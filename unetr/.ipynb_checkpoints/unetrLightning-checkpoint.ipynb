{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e8fef5",
   "metadata": {},
   "source": [
    "\n",
    "from typing import Any, List, OrderedDict, Tuple, Union\n",
    "\n",
    "from monai.networks.blocks import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.networks.nets import ViT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class UNETR(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Network that is similar to a U-Net for image segmentation with\n",
    "    an adaptation to use transformers and their attention mecanism.\n",
    "\n",
    "    Attributes:\n",
    "        num_layers: Number of layers in the Vision Transformer.\n",
    "        out_channels: Number of output channels.\n",
    "        patch_size: Size of the patch (tuple of `feature_size`) for the embedding in the transformer.\n",
    "        feat_size: Number of patches that can be put in a single image.\n",
    "        feature_size: Size of the feature.\n",
    "        hidden_size: Dimension of hidden layer.\n",
    "        classification: Boolean that represents if we are in a classification problem in the Vision Transformer.\n",
    "        vit: Vision Transformer block.\n",
    "        encoder1: First downsampling block (linked to the hidden states of the `vit`).\n",
    "        encoder2: Second downsampling block (linked to the hidden states of the `vit`).\n",
    "        encoder3: Third downsampling block (linked to the hidden states of the `vit`).\n",
    "        encoder4: Fourth downsampling block (linked to the hidden states of the `vit`).\n",
    "        decoder5: First upsampling block (linked to `vit` and `encoder4` outputs).\n",
    "        decoder4: Second upsampling block (linked to `encoder3` and `decoder5` outputs).\n",
    "        decoder3: Third upsampling block (linked to `encoder2` and `decoder4` outputs).\n",
    "        decoder2: Fourth upsampling block (linked to `encoder1` and `decoder3` outputs).\n",
    "        out: Output block (take only `decoder2` output).\n",
    "\n",
    "    References:\n",
    "        \"Hatamizadeh et al., UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        img_size: Tuple[int, int, int],\n",
    "        feature_size: int = 16,\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"perceptron\",\n",
    "        norm_name: Union[Tuple, str] = \"instance\",\n",
    "        conv_block: bool = False,\n",
    "        res_block: bool = True,\n",
    "        dropout_rate: float = 0.0,\n",
    "    ) -> \"UNETR\":\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            in_channels: Dimension of input channels.\n",
    "            out_channels: Dimension of output channels.\n",
    "            img_size: Dimension of input image.\n",
    "            feature_size: Dimension of network feature size.\n",
    "            hidden_size: Dimension of hidden layer.\n",
    "            mlp_dim: Dimension of feedforward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "            pos_embed: Position embedding layer type.\n",
    "            norm_name: Feature normalization type and arguments.\n",
    "            conv_block: Bool argument to determine if convolutional block is used.\n",
    "            res_block: Bool argument to determine if residual block is used.\n",
    "            dropout_rate: Fraction of the input units to drop.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # for single channel input 4-channel output with patch size of (96,96,96), feature size of 32 and batch norm\n",
    "            >>> net = UNETR(in_channels=1, out_channels=4, img_size=(96,96,96), feature_size=32, norm_name='batch')\n",
    "\n",
    "            # for 4-channel input 3-channel output with patch size of (128,128,128), conv position embedding and instance norm\n",
    "            >>> net = UNETR(in_channels=4, out_channels=3, img_size=(128,128,128), pos_embed='conv', norm_name='instance')\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: when dropout_rate is not between 0 and 1 or\n",
    "                hidden_size is not divisible by num_heads (needed for transformer blocks).\n",
    "            KeyError: when a wrong value of pos_embed is passed.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise AssertionError(\"dropout_rate should be between 0 and 1.\")\n",
    "\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise AssertionError(\"hidden size should be divisible by num_heads.\")\n",
    "\n",
    "        if pos_embed not in [\"conv\", \"perceptron\"]:\n",
    "            raise KeyError(f\"Position embedding layer of type {pos_embed} is not supported.\")\n",
    "\n",
    "        self.num_layers = 12\n",
    "        self.out_channels = out_channels\n",
    "        self.patch_size = (feature_size, feature_size, feature_size)\n",
    "        self.feat_size = (\n",
    "            img_size[0] // self.patch_size[0],\n",
    "            img_size[1] // self.patch_size[1],\n",
    "            img_size[2] // self.patch_size[2],\n",
    "        )\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.classification = False\n",
    "        self.vit = ViT(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=self.patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=num_heads,\n",
    "            pos_embed=pos_embed,\n",
    "            classification=self.classification,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder2 = UnetrPrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 2,\n",
    "            num_layer=2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder3 = UnetrPrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 4,\n",
    "            num_layer=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder4 = UnetrPrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            num_layer=0,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.out = UnetOutBlock(spatial_dims=3, in_channels=feature_size, out_channels=out_channels)  # type: ignore\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        pretrained_model_state_dict: OrderedDict[str, torch.Tensor],\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        img_size: Tuple[int, int, int],\n",
    "        new_out_channels: int = 0,\n",
    "        number_of_blocks_to_tune: int = 0,\n",
    "        feature_size: int = 16,\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"perceptron\",\n",
    "        norm_name: Union[Tuple, str] = \"instance\",\n",
    "        conv_block: bool = False,\n",
    "        res_block: bool = True,\n",
    "        dropout_rate: float = 0.0,\n",
    "    ) -> \"UNETR\":\n",
    "        \"\"\"Load networks weight from a pretrained model.\n",
    "        \n",
    "        In this method, we can easily perform a transformation of the network to make \n",
    "        a finetuning (modify last layer and reinitialize multiple blocks weights).\n",
    "\n",
    "        Arguments:\n",
    "            pretrained_model_state_dict: State dict of the pretrained model (need to be separetaly load).\n",
    "            in_channels: Dimension of input channels.\n",
    "            out_channels: Dimension of output channels.\n",
    "            img_size: Dimension of input image.\n",
    "            new_out_channels: Dimension of the new output channels (for finetuning).\n",
    "            number_of_blocks_to_tune: Number of blocks to tune (for finetuning).\n",
    "            feature_size: Dimension of network feature size.\n",
    "            hidden_size: Dimension of hidden layer.\n",
    "            mlp_dim: Dimension of feedforward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "            pos_embed: Position embedding layer type.\n",
    "            norm_name: Feature normalization type and arguments.\n",
    "            conv_block: Bool argument to determine if convolutional block is used.\n",
    "            res_block: Bool argument to determine if residual block is used.\n",
    "            dropout_rate: Fraction of the input units to drop.\n",
    "        \n",
    "        Raises:\n",
    "            AssertionError: \n",
    "                - When `new_out_channels` is positive but `number_of_blocks_to_tune`\n",
    "                is not positive (cannot change last block if we doesn't want to tune any block).\n",
    "                - When `number_of_blocks_to_tune` is greater than 10, because there are only 10 blocks.\n",
    "        \"\"\"\n",
    "        if new_out_channels > 0:\n",
    "            assert number_of_blocks_to_tune > 0, \"To change the last block, you need to authorize to tune it. Please choose a positive value (0 excluded)\"\n",
    "\n",
    "        if number_of_blocks_to_tune > 0:\n",
    "            assert number_of_blocks_to_tune <= 10, \"Too much block to tune. Please choose a number between 0 and 10 included\"\n",
    "\n",
    "        # creation and model loading\n",
    "        model = cls(in_channels, out_channels, img_size, feature_size=feature_size,\n",
    "                    hidden_size=hidden_size, mlp_dim=mlp_dim, num_heads=num_heads,\n",
    "                    pos_embed=pos_embed, norm_name=norm_name, conv_block=conv_block,\n",
    "                    res_block=res_block, dropout_rate=dropout_rate)\n",
    "        model.load_state_dict(pretrained_model_state_dict)\n",
    "\n",
    "        # finetuning of the model\n",
    "        if number_of_blocks_to_tune > 0:\n",
    "            model.number_of_blocks_to_tune = number_of_blocks_to_tune\n",
    "            \n",
    "            # change number of output channels\n",
    "            if out_channels != new_out_channels:\n",
    "                model.out = UnetOutBlock(spatial_dims=3, in_channels=feature_size, out_channels=new_out_channels)\n",
    "                model.out_channels = new_out_channels\n",
    "            \n",
    "            # reinitialize all blocks to tune\n",
    "            model.reinit_weights()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    @property\n",
    "    def backbone(self) -> List[nn.Module]:\n",
    "        \"\"\"Returns the part of the network that corresponding to\n",
    "        the backbone network to reuse for finetuning.\n",
    "        \n",
    "        Returns:\n",
    "            blocks: Network parts in a list.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: Raised when `number_of_blocks_to_tune` attribute is undefined,\n",
    "                in other words, when we are not in a finetuning.\n",
    "        \n",
    "        See also:\n",
    "            _get_blocks\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"number_of_blocks_to_tune\"):\n",
    "            raise AttributeError(\"you're not in fintuning the model\")\n",
    "        \n",
    "        return self._get_blocks()\n",
    "    \n",
    "    def _get_blocks(self, to_not_tune: bool = True) -> List[nn.Module]:\n",
    "        \"\"\"Gets blocks of the network.\n",
    "\n",
    "        If to_not_tune is activated, only the blocks that we doesn't want to\n",
    "        tune will be returned. Else, all other ones.\n",
    "        \n",
    "        Arguments:\n",
    "            to_not_tune: Represents the fact that the method will return blocks to tune or not.\n",
    "        \n",
    "        Returns:\n",
    "            blocks: Network blocks according to arguments.\n",
    "        \"\"\"\n",
    "        blocks = [self.vit, self.encoder1, self.encoder2, self.encoder3, self.encoder4, self.decoder5, self.decoder4, self.decoder3, self.decoder2, self.out]\n",
    "\n",
    "        # get only blocks to not tune\n",
    "        if to_not_tune:\n",
    "            return blocks[:len(blocks) - self.number_of_blocks_to_tune]\n",
    "        \n",
    "        # get blocks we want to tune\n",
    "        if self.number_of_blocks_to_tune <= 0:\n",
    "            return blocks\n",
    "        return blocks[len(blocks) - self.number_of_blocks_to_tune:]\n",
    "    \n",
    "    def reinit_weights(self) -> None:\n",
    "        \"\"\"Reinitializes the parameters weights of the right part\n",
    "        of the network/model following distributions.\n",
    "\n",
    "        You can view the association between layer types and distributions below:\n",
    "        - for the filters in convolutional layers, we use the Kaiming uniform initializer [1];\n",
    "        - for the biases in convolutional layers, we reinit to zeros.\n",
    "\n",
    "        References:\n",
    "            [1] \"He et al., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification <https://arxiv.org/abs/1502.01852>\"\n",
    "        \"\"\"\n",
    "        for block in self._get_blocks(to_not_tune=False):\n",
    "            for name, param in block.named_parameters():\n",
    "                if \"conv.weight\" in name:\n",
    "                    torch.nn.init.kaiming_uniform_(param) # better than Xavier or default values\n",
    "                elif \"conv.bias\" in name:\n",
    "                    torch.nn.init.zeros_(param)\n",
    "\n",
    "    def proj_feat(self, x: torch.Tensor, hidden_size: int, feat_size: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        \"\"\"Computes a feature projection.\n",
    "\n",
    "        The goal of this method is to change the way that we have to see\n",
    "        the `x` tensor by changing its dimensions. A permutation of axis is\n",
    "        realized to put temporal dimension second after batch and before\n",
    "        slices.\n",
    "        \n",
    "        Arguments:\n",
    "            x: Tensor to project.\n",
    "            hidden_size: Output size of the hidden layer.\n",
    "            feat_size: Size of the feature.\n",
    "\n",
    "        Returns:\n",
    "            x: New view of `x` tensor.\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), feat_size[0], feat_size[1], feat_size[2], hidden_size)\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_in: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Realizes the forward to make prediction.\n",
    "        \n",
    "        Arguments:\n",
    "            x_in: Tensor data to predict.\n",
    "        \n",
    "        Returns:\n",
    "            logits: Predictions tensor.\n",
    "        \"\"\"\n",
    "        x, hidden_states_out = self.vit(x_in)\n",
    "        enc1 = self.encoder1(x_in)\n",
    "        x2 = hidden_states_out[3]\n",
    "        enc2 = self.encoder2(self.proj_feat(x2, self.hidden_size, self.feat_size))\n",
    "        x3 = hidden_states_out[6]\n",
    "        enc3 = self.encoder3(self.proj_feat(x3, self.hidden_size, self.feat_size))\n",
    "        x4 = hidden_states_out[9]\n",
    "        enc4 = self.encoder4(self.proj_feat(x4, self.hidden_size, self.feat_size))\n",
    "        dec4 = self.proj_feat(x, self.hidden_size, self.feat_size)\n",
    "        dec3 = self.decoder5(dec4, enc4)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        out = self.decoder2(dec1, enc1)\n",
    "        logits = self.out(out)\n",
    "        return logits\n",
    "\n",
    "    def print_parameters(self, **print_kwargs: Any) -> None:\n",
    "        \"\"\"Prints in the console all the network parameters.\n",
    "        \n",
    "        All the associated names and tensor parameters\n",
    "        are printed to the console to check the parameters\n",
    "        sizes or values.\n",
    "\n",
    "        Arguments:\n",
    "            print_kwargs: Keyword arguments to pass to the print function.\n",
    "        \"\"\"\n",
    "        for name, params in self.named_parameters():\n",
    "            print(name, params.size(), params, **print_kwargs)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73e87de",
   "metadata": {},
   "source": [
    "model = UNETR.load_from_checkpoint(\"/home/aurelien/Documents/Segmentation Métastases cérébrales et méningiomes par IA/UNETR/metastase_IA/RunAll2/checkpoints/checkpoint-epoch=0829-val_loss=0.454.ckpt\")\n",
    "#model.eval()\n",
    "#y_hat = model(\"/home/aurelien/Documents/Segmentation Métastases cérébrales et méningiomes par IA/UNETR/metastase_IA/nifti_dataset/202207102/image.nii.gz\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd27b9",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
