{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e8fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any, List, OrderedDict, Tuple, Union\n",
    "\n",
    "from monai.networks.blocks import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.networks.nets import ViT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class UNETR(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Network that is similar to a U-Net for image segmentation with\n",
    "    an adaptation to use transformers and their attention mecanism.\n",
    "\n",
    "    Attributes:\n",
    "        num_layers: Number of layers in the Vision Transformer.\n",
    "        out_channels: Number of output channels.\n",
    "        patch_size: Size of the patch (tuple of `feature_size`) for the embedding in the transformer.\n",
    "        feat_size: Number of patches that can be put in a single image.\n",
    "        feature_size: Size of the feature.\n",
    "        hidden_size: Dimension of hidden layer.\n",
    "        classification: Boolean that represents if we are in a classification problem in the Vision Transformer.\n",
    "        vit: Vision Transformer block.\n",
    "        encoder1: First downsampling block (linked to the hidden states of the `vit`).\n",
    "        encoder2: Second downsampling block (linked to the hidden states of the `vit`).\n",
    "        encoder3: Third downsampling block (linked to the hidden states of the `vit`).\n",
    "        encoder4: Fourth downsampling block (linked to the hidden states of the `vit`).\n",
    "        decoder5: First upsampling block (linked to `vit` and `encoder4` outputs).\n",
    "        decoder4: Second upsampling block (linked to `encoder3` and `decoder5` outputs).\n",
    "        decoder3: Third upsampling block (linked to `encoder2` and `decoder4` outputs).\n",
    "        decoder2: Fourth upsampling block (linked to `encoder1` and `decoder3` outputs).\n",
    "        out: Output block (take only `decoder2` output).\n",
    "\n",
    "    References:\n",
    "        \"Hatamizadeh et al., UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        img_size: Tuple[int, int, int],\n",
    "        feature_size: int = 16,\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"perceptron\",\n",
    "        norm_name: Union[Tuple, str] = \"instance\",\n",
    "        conv_block: bool = False,\n",
    "        res_block: bool = True,\n",
    "        dropout_rate: float = 0.0,\n",
    "    ) -> \"UNETR\":\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            in_channels: Dimension of input channels.\n",
    "            out_channels: Dimension of output channels.\n",
    "            img_size: Dimension of input image.\n",
    "            feature_size: Dimension of network feature size.\n",
    "            hidden_size: Dimension of hidden layer.\n",
    "            mlp_dim: Dimension of feedforward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "            pos_embed: Position embedding layer type.\n",
    "            norm_name: Feature normalization type and arguments.\n",
    "            conv_block: Bool argument to determine if convolutional block is used.\n",
    "            res_block: Bool argument to determine if residual block is used.\n",
    "            dropout_rate: Fraction of the input units to drop.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # for single channel input 4-channel output with patch size of (96,96,96), feature size of 32 and batch norm\n",
    "            >>> net = UNETR(in_channels=1, out_channels=4, img_size=(96,96,96), feature_size=32, norm_name='batch')\n",
    "\n",
    "            # for 4-channel input 3-channel output with patch size of (128,128,128), conv position embedding and instance norm\n",
    "            >>> net = UNETR(in_channels=4, out_channels=3, img_size=(128,128,128), pos_embed='conv', norm_name='instance')\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: when dropout_rate is not between 0 and 1 or\n",
    "                hidden_size is not divisible by num_heads (needed for transformer blocks).\n",
    "            KeyError: when a wrong value of pos_embed is passed.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise AssertionError(\"dropout_rate should be between 0 and 1.\")\n",
    "\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise AssertionError(\"hidden size should be divisible by num_heads.\")\n",
    "\n",
    "        if pos_embed not in [\"conv\", \"perceptron\"]:\n",
    "            raise KeyError(f\"Position embedding layer of type {pos_embed} is not supported.\")\n",
    "\n",
    "        self.num_layers = 12\n",
    "        self.out_channels = out_channels\n",
    "        self.patch_size = (feature_size, feature_size, feature_size)\n",
    "        self.feat_size = (\n",
    "            img_size[0] // self.patch_size[0],\n",
    "            img_size[1] // self.patch_size[1],\n",
    "            img_size[2] // self.patch_size[2],\n",
    "        )\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.classification = False\n",
    "        self.vit = ViT(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=self.patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=num_heads,\n",
    "            pos_embed=pos_embed,\n",
    "            classification=self.classification,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder2 = UnetrPrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 2,\n",
    "            num_layer=2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder3 = UnetrPrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 4,\n",
    "            num_layer=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder4 = UnetrPrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            num_layer=0,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.out = UnetOutBlock(spatial_dims=3, in_channels=feature_size, out_channels=out_channels)  # type: ignore\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        pretrained_model_state_dict: OrderedDict[str, torch.Tensor],\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        img_size: Tuple[int, int, int],\n",
    "        new_out_channels: int = 0,\n",
    "        number_of_blocks_to_tune: int = 0,\n",
    "        feature_size: int = 16,\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"perceptron\",\n",
    "        norm_name: Union[Tuple, str] = \"instance\",\n",
    "        conv_block: bool = False,\n",
    "        res_block: bool = True,\n",
    "        dropout_rate: float = 0.0,\n",
    "    ) -> \"UNETR\":\n",
    "        \"\"\"Load networks weight from a pretrained model.\n",
    "        \n",
    "        In this method, we can easily perform a transformation of the network to make \n",
    "        a finetuning (modify last layer and reinitialize multiple blocks weights).\n",
    "\n",
    "        Arguments:\n",
    "            pretrained_model_state_dict: State dict of the pretrained model (need to be separetaly load).\n",
    "            in_channels: Dimension of input channels.\n",
    "            out_channels: Dimension of output channels.\n",
    "            img_size: Dimension of input image.\n",
    "            new_out_channels: Dimension of the new output channels (for finetuning).\n",
    "            number_of_blocks_to_tune: Number of blocks to tune (for finetuning).\n",
    "            feature_size: Dimension of network feature size.\n",
    "            hidden_size: Dimension of hidden layer.\n",
    "            mlp_dim: Dimension of feedforward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "            pos_embed: Position embedding layer type.\n",
    "            norm_name: Feature normalization type and arguments.\n",
    "            conv_block: Bool argument to determine if convolutional block is used.\n",
    "            res_block: Bool argument to determine if residual block is used.\n",
    "            dropout_rate: Fraction of the input units to drop.\n",
    "        \n",
    "        Raises:\n",
    "            AssertionError: \n",
    "                - When `new_out_channels` is positive but `number_of_blocks_to_tune`\n",
    "                is not positive (cannot change last block if we doesn't want to tune any block).\n",
    "                - When `number_of_blocks_to_tune` is greater than 10, because there are only 10 blocks.\n",
    "        \"\"\"\n",
    "        if new_out_channels > 0:\n",
    "            assert number_of_blocks_to_tune > 0, \"To change the last block, you need to authorize to tune it. Please choose a positive value (0 excluded)\"\n",
    "\n",
    "        if number_of_blocks_to_tune > 0:\n",
    "            assert number_of_blocks_to_tune <= 10, \"Too much block to tune. Please choose a number between 0 and 10 included\"\n",
    "\n",
    "        # creation and model loading\n",
    "        model = cls(in_channels, out_channels, img_size, feature_size=feature_size,\n",
    "                    hidden_size=hidden_size, mlp_dim=mlp_dim, num_heads=num_heads,\n",
    "                    pos_embed=pos_embed, norm_name=norm_name, conv_block=conv_block,\n",
    "                    res_block=res_block, dropout_rate=dropout_rate)\n",
    "        model.load_state_dict(pretrained_model_state_dict)\n",
    "\n",
    "        # finetuning of the model\n",
    "        if number_of_blocks_to_tune > 0:\n",
    "            model.number_of_blocks_to_tune = number_of_blocks_to_tune\n",
    "            \n",
    "            # change number of output channels\n",
    "            if out_channels != new_out_channels:\n",
    "                model.out = UnetOutBlock(spatial_dims=3, in_channels=feature_size, out_channels=new_out_channels)\n",
    "                model.out_channels = new_out_channels\n",
    "            \n",
    "            # reinitialize all blocks to tune\n",
    "            model.reinit_weights()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    @property\n",
    "    def backbone(self) -> List[nn.Module]:\n",
    "        \"\"\"Returns the part of the network that corresponding to\n",
    "        the backbone network to reuse for finetuning.\n",
    "        \n",
    "        Returns:\n",
    "            blocks: Network parts in a list.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: Raised when `number_of_blocks_to_tune` attribute is undefined,\n",
    "                in other words, when we are not in a finetuning.\n",
    "        \n",
    "        See also:\n",
    "            _get_blocks\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"number_of_blocks_to_tune\"):\n",
    "            raise AttributeError(\"you're not in fintuning the model\")\n",
    "        \n",
    "        return self._get_blocks()\n",
    "    \n",
    "    def _get_blocks(self, to_not_tune: bool = True) -> List[nn.Module]:\n",
    "        \"\"\"Gets blocks of the network.\n",
    "\n",
    "        If to_not_tune is activated, only the blocks that we doesn't want to\n",
    "        tune will be returned. Else, all other ones.\n",
    "        \n",
    "        Arguments:\n",
    "            to_not_tune: Represents the fact that the method will return blocks to tune or not.\n",
    "        \n",
    "        Returns:\n",
    "            blocks: Network blocks according to arguments.\n",
    "        \"\"\"\n",
    "        blocks = [self.vit, self.encoder1, self.encoder2, self.encoder3, self.encoder4, self.decoder5, self.decoder4, self.decoder3, self.decoder2, self.out]\n",
    "\n",
    "        # get only blocks to not tune\n",
    "        if to_not_tune:\n",
    "            return blocks[:len(blocks) - self.number_of_blocks_to_tune]\n",
    "        \n",
    "        # get blocks we want to tune\n",
    "        if self.number_of_blocks_to_tune <= 0:\n",
    "            return blocks\n",
    "        return blocks[len(blocks) - self.number_of_blocks_to_tune:]\n",
    "    \n",
    "    def reinit_weights(self) -> None:\n",
    "        \"\"\"Reinitializes the parameters weights of the right part\n",
    "        of the network/model following distributions.\n",
    "\n",
    "        You can view the association between layer types and distributions below:\n",
    "        - for the filters in convolutional layers, we use the Kaiming uniform initializer [1];\n",
    "        - for the biases in convolutional layers, we reinit to zeros.\n",
    "\n",
    "        References:\n",
    "            [1] \"He et al., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification <https://arxiv.org/abs/1502.01852>\"\n",
    "        \"\"\"\n",
    "        for block in self._get_blocks(to_not_tune=False):\n",
    "            for name, param in block.named_parameters():\n",
    "                if \"conv.weight\" in name:\n",
    "                    torch.nn.init.kaiming_uniform_(param) # better than Xavier or default values\n",
    "                elif \"conv.bias\" in name:\n",
    "                    torch.nn.init.zeros_(param)\n",
    "\n",
    "    def proj_feat(self, x: torch.Tensor, hidden_size: int, feat_size: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        \"\"\"Computes a feature projection.\n",
    "\n",
    "        The goal of this method is to change the way that we have to see\n",
    "        the `x` tensor by changing its dimensions. A permutation of axis is\n",
    "        realized to put temporal dimension second after batch and before\n",
    "        slices.\n",
    "        \n",
    "        Arguments:\n",
    "            x: Tensor to project.\n",
    "            hidden_size: Output size of the hidden layer.\n",
    "            feat_size: Size of the feature.\n",
    "\n",
    "        Returns:\n",
    "            x: New view of `x` tensor.\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), feat_size[0], feat_size[1], feat_size[2], hidden_size)\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_in: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Realizes the forward to make prediction.\n",
    "        \n",
    "        Arguments:\n",
    "            x_in: Tensor data to predict.\n",
    "        \n",
    "        Returns:\n",
    "            logits: Predictions tensor.\n",
    "        \"\"\"\n",
    "        x, hidden_states_out = self.vit(x_in)\n",
    "        enc1 = self.encoder1(x_in)\n",
    "        x2 = hidden_states_out[3]\n",
    "        enc2 = self.encoder2(self.proj_feat(x2, self.hidden_size, self.feat_size))\n",
    "        x3 = hidden_states_out[6]\n",
    "        enc3 = self.encoder3(self.proj_feat(x3, self.hidden_size, self.feat_size))\n",
    "        x4 = hidden_states_out[9]\n",
    "        enc4 = self.encoder4(self.proj_feat(x4, self.hidden_size, self.feat_size))\n",
    "        dec4 = self.proj_feat(x, self.hidden_size, self.feat_size)\n",
    "        dec3 = self.decoder5(dec4, enc4)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        out = self.decoder2(dec1, enc1)\n",
    "        logits = self.out(out)\n",
    "        return logits\n",
    "\n",
    "    def print_parameters(self, **print_kwargs: Any) -> None:\n",
    "        \"\"\"Prints in the console all the network parameters.\n",
    "        \n",
    "        All the associated names and tensor parameters\n",
    "        are printed to the console to check the parameters\n",
    "        sizes or values.\n",
    "\n",
    "        Arguments:\n",
    "            print_kwargs: Keyword arguments to pass to the print function.\n",
    "        \"\"\"\n",
    "        for name, params in self.named_parameters():\n",
    "            print(name, params.size(), params, **print_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73e87de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mUNETR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/aurelien/Documents/Segmentation Métastases cérébrales et méningiomes par IA/UNETR/metastase_IA/RunAll2/checkpoints/checkpoint-epoch=0829-val_loss=0.454.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#model.eval()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#y_hat = model(\"/home/aurelien/Documents/Segmentation Métastases cérébrales et méningiomes par IA/UNETR/metastase_IA/nifti_dataset/202207102/image.nii.gz\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:137\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     65\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:  \u001b[38;5;66;03m# type: ignore[valid-type]\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:158\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     map_location \u001b[38;5;241m=\u001b[39m cast(_MAP_LOCATION_TYPE, \u001b[38;5;28;01mlambda\u001b[39;00m storage, loc: storage)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m--> 158\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hparams_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     extension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(hparams_file)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/site-packages/lightning_lite/utilities/cloud_io.py:48\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     46\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/site-packages/torch/serialization.py:607\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    606\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 607\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/site-packages/torch/serialization.py:882\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    881\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m--> 882\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/pickle.py:1528\u001b[0m, in \u001b[0;36m_Unpickler.load_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1526\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1527\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1528\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(klass)\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/site-packages/torch/serialization.py:875\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_class\u001b[39m(\u001b[38;5;28mself\u001b[39m, mod_name, name):\n\u001b[1;32m    874\u001b[0m     mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m--> 875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/site-packages/pytorch_lightning/_graveyard/legacy_import_unpickler.py:24\u001b[0m, in \u001b[0;36mRedirectingUnpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;241m!=\u001b[39m new_module:\n\u001b[1;32m     23\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRedirecting import of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ConvData/lib/python3.9/pickle.py:1579\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING:\n\u001b[1;32m   1578\u001b[0m         module \u001b[38;5;241m=\u001b[39m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[0;32m-> 1579\u001b[0m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'meta'"
     ]
    }
   ],
   "source": [
    "model = UNETR.load_from_checkpoint(\"/home/aurelien/Documents/Segmentation Métastases cérébrales et méningiomes par IA/UNETR/metastase_IA/RunAll2/checkpoints/checkpoint-epoch=0829-val_loss=0.454.ckpt\")\n",
    "#model.eval()\n",
    "#y_hat = model(\"/home/aurelien/Documents/Segmentation Métastases cérébrales et méningiomes par IA/UNETR/metastase_IA/nifti_dataset/202207102/image.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd27b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
