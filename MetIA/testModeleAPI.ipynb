{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3153a786-8d12-4e93-a18a-9d108fac8722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:55:20) \n",
      "[Clang 16.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6724eee0-9f9f-4b62-b36c-3baa806481c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.backends' has no attribute 'mps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmps\u001b[49m\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      2\u001b[0m     mps_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mmps_device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.backends' has no attribute 'mps'"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9630fd7-ab08-43a0-a2f5-779b66ba8596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unetr.utilsUnetr.transforms import CropBedd, RandCropByPosNegLabeld, ResizeOrDoNothingd\n",
    "from functools import partial\n",
    "from monai.inferers import sliding_window_inference\n",
    "from unetr.networks.unetr import UNETR\n",
    "import os\n",
    "from unetr.model_module import SegmentationTask\n",
    "from monai.transforms import LoadImage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "import cv2\n",
    "import monai.transforms as transforms\n",
    "import scipy\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from monai import transforms\n",
    "from monai.transforms import LoadImage\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from pydicom import dcmread\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import rt_utils\n",
    "import pydicom\n",
    "from os import listdir\n",
    "import tqdm\n",
    "import json \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4746dea5-1cb8-4cbe-91ee-4e8b2be86cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def diceSpecSens(maskGT, maskPred):\n",
    "    predZone=np.where(maskPred==1.0, maskPred,10000)\n",
    "    gtZone=np.where(maskGT==1.0, maskGT,10000)\n",
    "    invPredZone=np.where(maskPred!=1.0, maskPred,10000)\n",
    "    \n",
    "    vp=np.where(gtZone==maskPred,1,0)\n",
    "    fn=np.where(maskPred-maskGT==-1,1,0)\n",
    "    fp=np.where(maskPred-maskGT==1,1,0)\n",
    "    vn=np.where(invPredZone==maskGT,1,0)\n",
    "    \n",
    "    print(np.sum(vp), np.sum(maskPred), np.sum(maskGT))\n",
    "    return (2*np.sum(vp))/(np.sum(maskPred)+np.sum(maskGT)), (np.sum(vn)/(np.sum(vn)+np.sum(fp))), (np.sum(vp)/(np.sum(vp)+np.sum(fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f9c84b2-304e-4d03-9415-4abb143408fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation():\n",
    "    dtype= torch.float32\n",
    "    voxel_space =(1.5, 1.5, 2.0)\n",
    "    a_min=-200.0\n",
    "    a_max=300\n",
    "    b_min=0.0\n",
    "    b_max=1.0\n",
    "    clip=True\n",
    "    crop_bed_max_number_of_rows_to_remove=0\n",
    "    crop_bed_max_number_of_cols_to_remove=0\n",
    "    crop_bed_min_spatial_size=(300, -1, -1)\n",
    "    enable_fgbg2indices_feature=False\n",
    "    pos=1.0\n",
    "    neg=1.0\n",
    "    num_samples=1\n",
    "    roi_size=(96, 96, 96)\n",
    "    random_flip_prob=0.2\n",
    "    random_90_deg_rotation_prob=0.2\n",
    "    random_intensity_scale_prob=0.1\n",
    "    random_intensity_shift_prob=0.1\n",
    "    val_resize=(-1, -1, 250)\n",
    "\n",
    "    spacing = transforms.Identity()\n",
    "    if all([space > 0.0 for space in voxel_space]):\n",
    "        spacing = transforms.Spacingd(\n",
    "            keys=[\"image\", \"label\"], pixdim=voxel_space, mode=(\"bilinear\", \"nearest\")\n",
    "        ) # to change the dimension of the voxel to have less data to compute\n",
    "\n",
    "        posneg_label_croper_kwargs = {\n",
    "                \"keys\": [\"image\", \"label\"],\n",
    "                \"label_key\": \"label\",\n",
    "                \"spatial_size\": roi_size,\n",
    "                \"pos\": pos,\n",
    "                \"neg\": neg,\n",
    "                \"num_samples\": num_samples,\n",
    "                \"image_key\": \"image\",\n",
    "                \"allow_smaller\": True,\n",
    "        }\n",
    "\n",
    "        fgbg2indices = transforms.Identity()\n",
    "        if enable_fgbg2indices_feature:\n",
    "            fgbg2indices = transforms.FgBgToIndicesd(\n",
    "                    keys=[\"image\", \"label\"], image_key=\"label\", image_threshold=0.0\n",
    "            ) # to crop samples close to the label mask\n",
    "            posneg_label_croper_kwargs[\"fg_indices_key\"] = \"image_fg_indices\"\n",
    "            posneg_label_croper_kwargs[\"bg_indices_key\"] = \"image_bg_indices\"\n",
    "        else:\n",
    "            posneg_label_croper_kwargs[\"image_threshold\"] = 0.0\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Orientationd(keys=[\"image\", \"label\"], axcodes=\"LAS\", allow_missing_keys=True), # to have the same orientation\n",
    "                    spacing,\n",
    "                    transforms.ScaleIntensityRanged(\n",
    "                        keys=[\"image\"], a_min=a_min, a_max=a_max, b_min=b_min, b_max=b_max, clip=clip, allow_missing_keys=True\n",
    "                    ), # scales image from a values to b values\n",
    "                    CropBedd(\n",
    "                        keys=[\"image\", \"label\"], image_key=\"image\",\n",
    "                        max_number_of_rows_to_remove=crop_bed_max_number_of_rows_to_remove,\n",
    "                        max_number_of_cols_to_remove=crop_bed_max_number_of_cols_to_remove,\n",
    "                        min_spatial_size=crop_bed_min_spatial_size,\n",
    "                        axcodes_orientation=\"LAS\",\n",
    "                    ), # crop the bed from the image (useless data)\n",
    "                    transforms.CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_missing_keys=True), # remove useless background image part\n",
    "                    fgbg2indices,\n",
    "                    transforms.RandFlipd(keys=[\"image\", \"label\"], prob=random_flip_prob, spatial_axis=0, allow_missing_keys=True), # random flip on the X axis\n",
    "                    transforms.RandFlipd(keys=[\"image\", \"label\"], prob=random_flip_prob, spatial_axis=1, allow_missing_keys=True), # random flip on the Y axis\n",
    "                    transforms.RandFlipd(keys=[\"image\", \"label\"], prob=random_flip_prob, spatial_axis=2, allow_missing_keys=True), # random flip on the Z axis\n",
    "                    transforms.RandRotate90d(keys=[\"image\", \"label\"], prob=random_90_deg_rotation_prob, max_k=3, allow_missing_keys=True), # random 90 degree rotation\n",
    "                    transforms.RandScaleIntensityd(keys=\"image\", factors=0.1, prob=random_intensity_scale_prob), # random intensity scale\n",
    "                    transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=random_intensity_shift_prob), # random intensity shifting\n",
    "                    transforms.ToTensord(keys=[\"image\", \"label\"], dtype=dtype), # to have a PyTorch tensor as output\n",
    "                ]\n",
    "            )\n",
    "    return transform\n",
    "\n",
    "def loadModel(pathModelFile):\n",
    "    map_location = torch.device('cpu')\n",
    "    model= SegmentationTask.load_from_checkpoint(pathModelFile, map_location=map_location)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def loadDicomImage(slices_folder):\n",
    "    image=torch.tensor([LoadImage(image_only=True)(slices_folder)])\n",
    "    image=((image/np.max(np.array(image)))*255)\n",
    "    return image\n",
    "\n",
    "def applyTransforms(transform, image):\n",
    "    print(\"applyT\", image.shape)\n",
    "    image={\"image\":image, \"label\":torch.zeros_like(image),\"patient_id\":'201905984', \"has_meta\":True}\n",
    "    image=transform(image)\n",
    "    print(\"applyT\", image['image'].shape)\n",
    "    return image\n",
    "\n",
    "def applyUNETR(dicoImage, model):\n",
    "    label =sliding_window_inference(inputs=dicoImage[\"image\"][None], \n",
    "                                            roi_size=(96, 96, 96), \n",
    "                                            sw_batch_size=4,\n",
    "                                            predictor=model,\n",
    "                                            overlap=0.5)\n",
    "\n",
    "    label = torch.argmax(label, dim=1, keepdim=True)\n",
    "    \n",
    "    size=label.shape\n",
    "    print(\"applyUNETR\", size[1], size[2], size[3], size[4])\n",
    "    dicoImage[\"label\"]=label.reshape((size[1], size[2], size[3], size[4]))\n",
    "    return dicoImage\n",
    "\n",
    "def disapplyTransforms(transform, dicoImage):\n",
    "    dicoImage = transform.inverse(dicoImage)\n",
    "    return dicoImage[\"label\"], dicoImage[\"image\"]\n",
    "\n",
    "\n",
    "\n",
    "def getLabelOfIRM_from_path(pathSlicesIRM, pathModelFile):\n",
    "    image = loadDicomImage(pathSlicesIRM)\n",
    "    transform = transformation()\n",
    "    dicoImage = applyTransforms(transform, image)\n",
    "    model = loadModel(pathModelFile)\n",
    "    dicoImage = applyUNETR(dicoImage, model)\n",
    "    label, imageT = disapplyTransforms(transform, dicoImage)\n",
    "    return image/255, label, imageT\n",
    "\n",
    "def getLabelOfIRM_from_image(image, mask, pathModelFile):\n",
    "    image=torch.tensor([image])\n",
    "    image=((image/np.max(np.array(image)))*255)\n",
    "    transform = transformation()\n",
    "    dicoImage = applyTransforms(transform, image)\n",
    "    model = loadModel(pathModelFile)\n",
    "    dicoImage = applyUNETR(dicoImage, model)\n",
    "    label, imageT = disapplyTransforms(transform, dicoImage)\n",
    "    return image/255, label, imageT\n",
    "\n",
    "#pathSlicesIRM='/home/aurelien/Documents/Segmentation Métastases cérébrales et méningiomes par IA/UNETR/metastase_IA/BaseDonnée220Patients/201905984/RM'\n",
    "#pathModelFile=\"/home/aurelien/Documents/Segmentation Métastases cérébrales et méningiomes par IA/UNETR/metastase_IA/RunAll4/checkpoints/checkpoint-epoch=1599-val_loss=0.225.ckpt\"\n",
    "\n",
    "#image, label, imageT = getLabelOfIRM(pathSlicesIRM, pathModelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14e08ef-6071-443b-a0ec-b967307ebee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miseEnDossier(patient_id):\n",
    "    path_newDirRM = os.path.join(meta_data_dir, patient_id, \"RM/\")\n",
    "    path_newDirMETA = os.path.join(meta_data_dir, patient_id, \"META/\")\n",
    "    if not os.path.exists(path_newDirRM):\n",
    "        os.mkdir(path_newDirRM)\n",
    "    if not os.path.exists(path_newDirMETA):\n",
    "        os.mkdir(path_newDirMETA)\n",
    "    for mr in glob.glob(os.path.join(meta_data_dir, patient_id, \"MR*\")):\n",
    "        path_dir, name_file= os.path.split(mr)\n",
    "        os.rename(mr, str(path_newDirRM)+str(name_file))\n",
    "    for rs in glob.glob(os.path.join(meta_data_dir, patient_id, \"RS*\")):\n",
    "        path_dir, name_file= os.path.split(rs)\n",
    "        os.rename(rs, str(path_newDirMETA)+str(name_file))\n",
    "        \n",
    "def fusionMask(mask):\n",
    "        if (type(mask)==list):\n",
    "            premMask = mask[0]\n",
    "            for autMask in mask[1:]:\n",
    "                premMask = np.where(autMask > 0, autMask, premMask)\n",
    "            return premMask\n",
    "        else:\n",
    "            return mask\n",
    "    \n",
    "listeName=[]\n",
    "def load_dicom_with_mask(slices_folder, mask_file):\n",
    "    rt_struct = rt_utils.RTStructBuilder.create_from(slices_folder, mask_file)\n",
    "    maskAllAlone=[]\n",
    "    for i in rt_struct.get_roi_names():\n",
    "        if \"GTV\" in i or \"recid\" in i or \"FRONTAL\" in i or \"GTC\" in i or \"Cerebelleux\" in i or \"PARIET\" in i or \"gtv\" in i or \"M1.\" in i or \"M2.\" in i or \"FRONTAL\" in i:\n",
    "            if \"patient\" not in i and \"POST OP\" not in i and \"Préop\" not in i and \"External\" not in i and \"cav\" not in i and \"Cav\" not in i and \"cavite\" not in i and \"cavité\" not in i and \"Cavité\" not in i and \"PTV\" not in i and \"ANCIEN\" not in i and \"Ancien\" not in i and \"ANC\" not in i and \"Anecien\" not in i and \"PTC\" not in i:\n",
    "                maskAllAlone.append(rt_struct.get_roi_mask_by_name(i).astype(np.float32))\n",
    "                listeName.append(i)\n",
    "    mask = fusionMask(maskAllAlone)\n",
    "    img = np.array(LoadImage( image_only=True)(slices_folder))\n",
    "\n",
    "    img = np.rot90(img)\n",
    "    img = np.flip(img, 0)\n",
    "    img = (img/np.max(img))*255\n",
    "    return img, np.array(mask)\n",
    "\n",
    "\n",
    "def load_dicom_series_without_mask(patient_folder):\n",
    "    img = LoadImage( image_only=True)(patient_folder)\n",
    "    mask = np.zeros_like(img)\n",
    "    return np.array(img), mask\n",
    "  \n",
    "def filter_patients_with_meta(patient_id: str) -> bool:\n",
    "    miseEnDossier(patient_id)\n",
    "    # if no folder in patient directory\n",
    "    if len(glob.glob(os.path.join(meta_data_dir, patient_id, \"*\"))) == 0:\n",
    "        return False\n",
    "\n",
    "    # if no named folders \"META\" or \"RM\"\n",
    "    if len(glob.glob(os.path.join(meta_data_dir, patient_id, \"META\"))) == 0 or len(glob.glob(os.path.join(meta_data_dir, patient_id, \"RM\"))) == 0:\n",
    "        return False\n",
    "    \n",
    "    # if no dicom image in the META folder\n",
    "    if len(glob.glob(os.path.join(meta_data_dir, patient_id, \"META\", \"*\"))) == 0:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def filter_patients_without_meta(patient_id: str) -> bool:\n",
    "    miseEnDossier(patient_id)\n",
    "    # if no folder in patient directory\n",
    "    if len(glob.glob(os.path.join(meta_data_dir, patient_id, \"*\"))) == 0:\n",
    "        return False\n",
    "\n",
    "    # if no named folders \"META\" or \"RM\"\n",
    "    if len(glob.glob(os.path.join(meta_data_dir, patient_id, \"RM\"))) == 0:\n",
    "        return False\n",
    "\n",
    "    if len(glob.glob(os.path.join(meta_data_dir, patient_id, \"META\"))) != 0:\n",
    "        if len(glob.glob(os.path.join(meta_data_dir, patient_id, \"META\", \"*\"))) != 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c3a447-3988-4e5c-9773-0733bed22d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applyT torch.Size([1, 512, 512, 208])\n",
      "applyT torch.Size([1, 342, 342, 104])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m pathModelFile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/romain/Downloads/Modeles_Pre_Entraines/checkpoint_epoch1599_val_loss0255.cpkt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Charger les étiquettes pour les images DICOM\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m image, label, imageT \u001b[38;5;241m=\u001b[39m \u001b[43mgetLabelOfIRM_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathSlicesIRM2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpathModelFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Afficher les résultats\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, image\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn [17], line 119\u001b[0m, in \u001b[0;36mgetLabelOfIRM_from_path\u001b[0;34m(pathSlicesIRM, pathModelFile)\u001b[0m\n\u001b[1;32m    117\u001b[0m transform \u001b[38;5;241m=\u001b[39m transformation()\n\u001b[1;32m    118\u001b[0m dicoImage \u001b[38;5;241m=\u001b[39m applyTransforms(transform, image)\n\u001b[0;32m--> 119\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathModelFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m dicoImage \u001b[38;5;241m=\u001b[39m applyUNETR(dicoImage, model)\n\u001b[1;32m    121\u001b[0m label, imageT \u001b[38;5;241m=\u001b[39m disapplyTransforms(transform, dicoImage)\n",
      "Cell \u001b[0;32mIn [17], line 79\u001b[0m, in \u001b[0;36mloadModel\u001b[0;34m(pathModelFile)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadModel\u001b[39m(pathModelFile):\n\u001b[1;32m     78\u001b[0m     map_location \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     model\u001b[38;5;241m=\u001b[39m \u001b[43mSegmentationTask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathModelFile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:137\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     65\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:  \u001b[38;5;66;03m# type: ignore[valid-type]\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:180\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:225\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 225\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "File \u001b[0;32m~/Documents/P_R_O_J_E_C_T_S/IRM-Project/BrainMetaSegmentatorUI-Back/MetIA/unetr/model_module.py:133\u001b[0m, in \u001b[0;36mSegmentationTask.__init__\u001b[0;34m(self, prediction_dir, test_validation_dir, pretrained_file_path, in_channels, out_channels, roi_size, new_out_channels, number_of_blocks_to_tune, feature_size, hidden_size, mlp_dim, num_heads, pos_embed, norm_name, conv_block, res_block, dropout_rate, infer_overlap, max_epochs, labels_names, labels_colors, smooth_dr, smooth_nr, sw_batch_size, use_bce_loss_when_binary_problem, save_max_n_batches, test_saving_type, prediction_saving_type, metrics, log_max_n_batches, val_test_logging_type, prediction_logging_type)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    prediction_dir: Directory to save prediction stage outputs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    prediction_logging_type: Type of logging for the prediction stage.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28msuper\u001b[39m(SegmentationTask, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m UNETR\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_file_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, in_channels, \n\u001b[1;32m    134\u001b[0m     out_channels, roi_size, new_out_channels\u001b[38;5;241m=\u001b[39mnew_out_channels,\n\u001b[1;32m    135\u001b[0m     number_of_blocks_to_tune\u001b[38;5;241m=\u001b[39mnumber_of_blocks_to_tune,\n\u001b[1;32m    136\u001b[0m     feature_size\u001b[38;5;241m=\u001b[39mfeature_size, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m    137\u001b[0m     mlp_dim\u001b[38;5;241m=\u001b[39mmlp_dim, num_heads\u001b[38;5;241m=\u001b[39mnum_heads, pos_embed\u001b[38;5;241m=\u001b[39mpos_embed, norm_name\u001b[38;5;241m=\u001b[39mnorm_name,\n\u001b[1;32m    138\u001b[0m     conv_block\u001b[38;5;241m=\u001b[39mconv_block, res_block\u001b[38;5;241m=\u001b[39mres_block, dropout_rate\u001b[38;5;241m=\u001b[39mdropout_rate,\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbackbone)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inferer \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    142\u001b[0m     sliding_window_inference,\n\u001b[1;32m    143\u001b[0m     roi_size\u001b[38;5;241m=\u001b[39mroi_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     overlap\u001b[38;5;241m=\u001b[39minfer_overlap\n\u001b[1;32m    147\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/torch/serialization.py:607\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    606\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 607\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/torch/serialization.py:882\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    881\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m--> 882\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/pickle.py:1253\u001b[0m, in \u001b[0;36m_Unpickler.load_binpersid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_binpersid\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1252\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/torch/serialization.py:857\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m data_type, key, location, size \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[0;32m--> 857\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m storage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/torch/serialization.py:846\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    843\u001b[0m dtype \u001b[38;5;241m=\u001b[39m data_type(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    845\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, size, dtype)\u001b[38;5;241m.\u001b[39mstorage()\n\u001b[0;32m--> 846\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/torch/serialization.py:151\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 151\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    153\u001b[0m             storage_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/projet_IRM_UI/lib/python3.9/site-packages/torch/serialization.py:135\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    132\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    136\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    137\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    139\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    140\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# Chemins vers les dossiers d'images DICOM et le modèle UNETR\n",
    "pathSlicesIRM = '/Users/romain/Documents/P_R_O_J_E_C_T_S/IRM-Project/mbiaDataDownloads/DATA_VERITE_TERRAIN/RM'\n",
    "pathSlicesIRM2 = '/Users/romain/Documents/P_R_O_J_E_C_T_S/IRM-Project/mbiaDataDownloads/nifti_dataset_small/201704321/image.nii.gz'\n",
    "pathModelFile = '/Users/romain/Downloads/Modeles_Pre_Entraines/checkpoint_epoch1599_val_loss0255.cpkt'\n",
    "\n",
    "# Charger les étiquettes pour les images DICOM\n",
    "image, label, imageT = getLabelOfIRM_from_path(pathSlicesIRM2, pathModelFile)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Label shape:\", label.shape)\n",
    "print(\"Transformed image shape:\", imageT.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
